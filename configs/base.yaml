# --- Data Configuration ---
data:
  dataset: "data.datasets.CholecSeg8k"
  batch_size: 8
  num_workers: 4
  pin_memory: true

# --- Model Configuration ---
model:
  checkpoint: null # continue training from this checkpoint
  finetune: null # finetune from this pre-trained model with frozen encoder

  encoder:
    name: "models.simple_unet.Encoder"
    params: {}

  decoders:
    segmentation:
      enabled: true
      name: "models.simple_unet.SegmentationDecoder"
      params:
        num_classes: 2

    disparity: 
      enabled: false
      name: null # TODO: Implement disparity decoder
      params: 
        num_classes: 1

# --- Training Configuration ---
training:
  epochs: 10

  criterion:
    name: "monai.losses.DiceCELoss"
    params:
      sigmoid: True

  optimizer:
    name: "torch.optim.AdamW"
    params:
      lr: 0.0001
      weight_decay: 0.01

  scheduler:
    name: "torch.optim.lr_scheduler.ReduceLROnPlateau"
    params:
      mode: "min"
      factor: 0.1
      patience: 10

  # --- Knowledge Distillation Configuration ---
  knowledge_distillation:
    enabled: false
    weight: 0.5

    teacher_paths:
      - "path/to/teacher1.pth"
      - "path/to/teacher2.pth"

    encoder:
      name: "models.simple_unet.Encoder"
      params: {}

    decoder:
      name: "models.simple_unet.SegmentationDecoder"
      params:
        num_classes: 2

misc:
  seed: 42